# .github/workflows/threads-crawler.yml
name: Daily Threads Crawler (7 AM KST)

on:
  schedule:
    - cron: '15 22 * * *'  # 7:15 AM KST (offset to avoid peak)
  workflow_dispatch:
    inputs:
      full_scrape:
        description: 'Run full scrape (default: incremental)'
        required: false
        default: false
        type: boolean

permissions:
  contents: read
  pages: write
  id-token: write

env:
  PYTHON_VERSION: '3.11'
  TARGET_URL: 'https://www.threads.com/?hl=ko'
  REPORT_URL: 'https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}'

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    outputs:
      scrape-status: ${{ steps.scrape.outcome }}
      report-path: ${{ steps.generate-report.outputs.report_path }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install playwright beautifulsoup4 jinja2
      
      - name: Get Playwright version for cache key
        id: playwright-version
        run: |
          PLAYWRIGHT_VERSION=$(python -c "import playwright; print(playwright.__version__)")
          echo "version=$PLAYWRIGHT_VERSION" >> $GITHUB_OUTPUT
      
      - name: Cache Playwright browsers
        uses: actions/cache@v4
        id: playwright-cache
        with:
          path: ~/.cache/ms-playwright
          key: playwright-${{ runner.os }}-${{ steps.playwright-version.outputs.version }}
      
      - name: Install Playwright (no cache)
        if: steps.playwright-cache.outputs.cache-hit != 'true'
        run: python -m playwright install chromium --with-deps
      
      - name: Install Playwright system deps (cache hit)
        if: steps.playwright-cache.outputs.cache-hit == 'true'
        run: python -m playwright install-deps chromium
      
      - name: Create output directories
        run: mkdir -p output screenshots reports
      
      - name: Run Threads scraper
        id: scrape
        run: python scripts/threads_scraper.py
        env:
          TARGET_URL: ${{ env.TARGET_URL }}
          CI: true
      
      - name: Generate reports
        id: generate-report
        run: |
          python scripts/generate_report.py
          echo "report_path=reports/index.html" >> $GITHUB_OUTPUT
      
      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: scrape-results-${{ github.run_number }}
          path: |
            output/
            screenshots/
            reports/
          retention-days: 14
      
      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: reports

  deploy-pages:
    needs: scrape
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

  notify-slack:
    needs: [scrape, deploy-pages]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Notify Slack - Success
        if: needs.scrape.result == 'success'
        uses: slackapi/slack-github-action@v2.0.0
        with:
          webhook: ${{ secrets.SLACK_WEBHOOK_URL }}
          webhook-type: incoming-webhook
          payload: |
            text: "âœ… Threads Crawler Completed Successfully"
            blocks:
              - type: "header"
                text:
                  type: "plain_text"
                  text: "âœ… Daily Threads Crawl Complete"
              - type: "section"
                fields:
                  - type: "mrkdwn"
                    text: "*Target:*\n${{ env.TARGET_URL }}"
                  - type: "mrkdwn"
                    text: "*Run ID:*\n${{ github.run_id }}"
                  - type: "mrkdwn"
                    text: "*Triggered:*\n${{ github.event_name }}"
                  - type: "mrkdwn"
                    text: "*Status:*\nSuccess âœ“"
              - type: "actions"
                elements:
                  - type: "button"
                    text:
                      type: "plain_text"
                      text: "ðŸ“Š View Report"
                    url: "${{ env.REPORT_URL }}"
                    style: "primary"
                  - type: "button"
                    text:
                      type: "plain_text"
                      text: "ðŸ“¦ Download Artifacts"
                    url: "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
      
      - name: Notify Slack - Failure
        if: needs.scrape.result == 'failure'
        uses: slackapi/slack-github-action@v2.0.0
        with:
          webhook: ${{ secrets.SLACK_WEBHOOK_URL }}
          webhook-type: incoming-webhook
          payload: |
            text: "âŒ Threads Crawler Failed"
            blocks:
              - type: "header"
                text:
                  type: "plain_text"
                  text: "âŒ Daily Threads Crawl Failed"
              - type: "section"
                text:
                  type: "mrkdwn"
                  text: "*The scheduled crawl encountered an error.*\nPlease check the workflow logs for details."
              - type: "section"
                fields:
                  - type: "mrkdwn"
                    text: "*Target:*\n${{ env.TARGET_URL }}"
                  - type: "mrkdwn"
                    text: "*Run ID:*\n${{ github.run_id }}"
              - type: "actions"
                elements:
                  - type: "button"
                    text:
                      type: "plain_text"
                      text: "ðŸ” View Logs"
                    url: "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                    style: "danger"

  analyze-with-claude:
    needs: scrape
    runs-on: ubuntu-latest
    if: needs.scrape.result == 'success'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Download scrape artifacts
        uses: actions/download-artifact@v4
        with:
          name: scrape-results-${{ github.run_number }}
          path: ./data

      - name: Analyze with Claude
        uses: anthropics/claude-code-action@v1
        env:
          # Hooksì—ì„œ Slack ì „ì†¡ ì‹œ ì‚¬ìš©í•  í™˜ê²½ë³€ìˆ˜ ì£¼ìž…
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          # --dangerously-skip-permissions: ë¬´ì¸ ì‹¤í–‰ì„ ìœ„í•´ í•„ìˆ˜ 
          claude_args: "--dangerously-skip-permissions  --output-format json"
          prompt: >
           Analyze the scraped Threads.com data in ./data/output/
            Generate a summary report including:
            - Top trending topics
            - Content themes
            - Notable posts or patterns
            Save analysis to ./data/reports/ai_analysis.md
      
      - name: Upload AI analysis
        uses: actions/upload-artifact@v4
        with:
          name: ai-analysis-${{ github.run_number }}
          path: ./data/reports/ai_analysis.md
