# .github/workflows/threads-crawler.yml
name: Daily Threads Crawler (7 AM KST)

on:
  schedule:
    - cron: '15 22 * * *'  # 7:15 AM KST (offset to avoid peak)
  workflow_dispatch:
    inputs:
      full_scrape:
        description: 'Run full scrape (default: incremental)'
        required: false
        default: 'false'
        type: boolean

permissions:
  contents: read
  pages: write
  id-token: write

env:
  PYTHON_VERSION: '3.11'
  TARGET_URL: 'https://www.threads.com/?hl=ko'
  REPORT_URL: 'https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}'

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    outputs:
      scrape-status: ${{ steps.scrape.outcome }}
      report-path: ${{ steps.generate-report.outputs.report_path }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      
      - name: Install dependencies
        run: |
          pip install playwright beautifulsoup4 jinja2
      
      - name: Get Playwright version for cache key
        id: playwright-version
        run: |
          PLAYWRIGHT_VERSION=$(pip show playwright | grep Version | cut -d' ' -f2)
          echo "version=$PLAYWRIGHT_VERSION" >> $GITHUB_OUTPUT
      
      - name: Cache Playwright browsers
        uses: actions/cache@v4
        id: playwright-cache
        with:
          path: ~/.cache/ms-playwright
          key: playwright-${{ runner.os }}-${{ hashFiles('requirements.txt') }}
      
      - name: Install Playwright browsers
        if: steps.playwright-cache.outputs.cache-hit != 'true'
        run: |
          pip install playwright
          python -m playwright install chromium --with-deps
      
      - name: Install Playwright system deps only (cache hit)
        if: steps.playwright-cache.outputs.cache-hit == 'true'
        run: python -m playwright install-deps chromium
      
      - name: Create output directories
        run: mkdir -p output screenshots reports
      
      - name: Run Threads scraper
        id: scrape
        run: python scripts/threads_scraper.py
        env:
          TARGET_URL: ${{ env.TARGET_URL }}
          CI: true
      
      - name: Generate reports
        id: generate-report
        run: |
          python scripts/generate_report.py
          echo "report_path=reports/index.html" >> $GITHUB_OUTPUT
      
      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: scrape-results-${{ github.run_number }}
          path: |
            output/
            screenshots/
            reports/
          retention-days: 14
      
      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: reports

  deploy-pages:
    needs: scrape
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

  notify-slack:
    needs: [scrape, deploy-pages]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Notify Slack - Success
        if: needs.scrape.result == 'success'
        uses: slackapi/slack-github-action@v2.0.0
        with:
          webhook: ${{ secrets.SLACK_WEBHOOK_URL }}
          webhook-type: incoming-webhook
          payload: |
            text: "‚úÖ Threads Crawler Completed Successfully"
            blocks:
              - type: "header"
                text:
                  type: "plain_text"
                  text: "‚úÖ Daily Threads Crawl Complete"
              - type: "section"
                fields:
                  - type: "mrkdwn"
                    text: "*Target:*\n${{ env.TARGET_URL }}"
                  - type: "mrkdwn"
                    text: "*Run ID:*\n${{ github.run_id }}"
                  - type: "mrkdwn"
                    text: "*Triggered:*\n${{ github.event_name }}"
                  - type: "mrkdwn"
                    text: "*Status:*\nSuccess ‚úì"
              - type: "actions"
                elements:
                  - type: "button"
                    text:
                      type: "plain_text"
                      text: "üìä View Report"
                    url: "${{ env.REPORT_URL }}"
                    style: "primary"
                  - type: "button"
                    text:
                      type: "plain_text"
                      text: "üì¶ Download Artifacts"
                    url: "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
      
      - name: Notify Slack - Failure
        if: needs.scrape.result == 'failure'
        uses: slackapi/slack-github-action@v2.0.0
        with:
          webhook: ${{ secrets.SLACK_WEBHOOK_URL }}
          webhook-type: incoming-webhook
          payload: |
            text: "‚ùå Threads Crawler Failed"
            blocks:
              - type: "header"
                text:
                  type: "plain_text"
                  text: "‚ùå Daily Threads Crawl Failed"
              - type: "section"
                text:
                  type: "mrkdwn"
                  text: "*The scheduled crawl encountered an error.*\nPlease check the workflow logs for details."
              - type: "section"
                fields:
                  - type: "mrkdwn"
                    text: "*Target:*\n${{ env.TARGET_URL }}"
                  - type: "mrkdwn"
                    text: "*Run ID:*\n${{ github.run_id }}"
              - type: "actions"
                elements:
                  - type: "button"
                    text:
                      type: "plain_text"
                      text: "üîç View Logs"
                    url: "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                    style: "danger"

  analyze-with-claude:
    needs: scrape
    runs-on: ubuntu-latest
    if: needs.scrape.result == 'success'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Download scrape artifacts
        uses: actions/download-artifact@v4
        with:
          name: scrape-results-${{ github.run_number }}
          path: ./data
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      - name: Install Claude Code
        run: npm install -g @anthropic-ai/claude-code@latest
      
      - name: Analyze with Claude
        run: |
          claude -p "
            Analyze the scraped Threads.com data in ./data/output/
            Generate a summary report including:
            - Top trending topics
            - Content themes
            - Notable posts or patterns
            Save analysis to ./data/reports/ai_analysis.md
          " --output-format json --max-turns 5
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      
      - name: Upload AI analysis
        uses: actions/upload-artifact@v4
        with:
          name: ai-analysis-${{ github.run_number }}
          path: ./data/reports/ai_analysis.md
